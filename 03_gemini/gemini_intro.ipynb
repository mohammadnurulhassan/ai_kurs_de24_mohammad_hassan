{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d641e6b",
   "metadata": {},
   "source": [
    "# Gemini   API Intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09edc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  Why did the data engineer bring a broom to work?\n",
      "    To sweep up all the **dirty data** from the data lake!\n",
      "\n",
      "2.  What's a data engineer's favorite bedtime story?\n",
      "    \"The Little Pipeline That Could... Until Midnight.\"\n",
      "\n",
      "3.  How many data engineers does it take to change a lightbulb?\n",
      "    None, they just build a **workflow** to automate the bulb replacement and then spend three weeks debugging why the light is still off.\n",
      "\n",
      "4.  What's a data engineer's favorite magic trick?\n",
      "    Making an entire downstream reporting dashboard disappear with a single `ALTER TABLE` statement.\n",
      "\n",
      "5.  What do you call a data engineer who never sleeps?\n",
      "    Just a data engineer.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate funny jokes about data engineering. Give 5 points in markdown format\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3737f12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are 5 funny jokes about data engineering:\\n\\n1.  Why did the data engineer bring a broom to work?\\n    To sweep up all the **dirty data** from the data lake!\\n\\n2.  What\\'s a data engineer\\'s favorite bedtime story?\\n    \"The Little Pipeline That Could... Until Midnight.\"\\n\\n3.  How many data engineers does it take to change a lightbulb?\\n    None, they just build a **workflow** to automate the bulb replacement and then spend three weeks debugging why the light is still off.\\n\\n4.  What\\'s a data engineer\\'s favorite magic trick?\\n    Making an entire downstream reporting dashboard disappear with a single `ALTER TABLE` statement.\\n\\n5.  What do you call a data engineer who never sleeps?\\n    Just a data engineer.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e95932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfda958",
   "metadata": {},
   "source": [
    "## Analyze Tokens\n",
    "\n",
    "- basic units of text for LLMs\n",
    "- can be short as one character or as long as one word\n",
    "\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5Flash)\n",
    "- Request per minute (RPM):10\n",
    "- Tokens per minute(TPM): 250000\n",
    "- Request per day (RPD): 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed418f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=171,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=2018,\n",
       "  total_token_count=2204\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata= response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00e90e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  pydantic import  BaseModel\n",
    "\n",
    "isinstance(response, BaseModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10525b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output token count -number of tokens  in models response\n",
      "metadata.candidates_token_count=171\n"
     ]
    }
   ],
   "source": [
    "print (\"Output token count -number of tokens  in models response\")\n",
    "print(f\"{metadata.candidates_token_count=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a22f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token count -number of tokens  in prompt\n",
      "metadata.prompt_token_count=15\n"
     ]
    }
   ],
   "source": [
    "print (\"Input token count -number of tokens  in prompt\")\n",
    "print(f\"{metadata.prompt_token_count=}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561cb785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=151,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1712,\n",
       "  total_token_count=1878\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a948d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used  for internal thinking\n",
      "metadata.thoughts_token_count=2018\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokens used  for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5428e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used  for this request\n",
      "metadata.total_token_count=2204\n"
     ]
    }
   ],
   "source": [
    "print (\"Total tokens used  for this request\")\n",
    "print(f\"{metadata.total_token_count=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d914e52",
   "metadata": {},
   "source": [
    "## Thinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d019cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  A data engineer, a software engineer, and a DevOps engineer walk into a bar. The data engineer immediately asks, \"Where's the nearest real-time data tap for the drink orders?\" The software engineer replies, \"Why not just use the existing API?\" The DevOps engineer just sighs and starts setting up a monitoring dashboard for the beer taps.\n",
      "\n",
      "2.  Why did the data pipeline break up with the data lake? It said, \"You're too messy, and I have serious commitment issues with your schema-on-read philosophy!\"\n",
      "\n",
      "3.  What's a data engineer's favorite type of music? Anything with a strong \"ETL beat\" and \"streaming\" capabilities. They also love to \"transform\" old records into new hits!\n",
      "\n",
      "4.  A new junior data engineer asks their senior, \"What's the difference between a data warehouse and a data mart?\" The senior replies, \"About a dozen meetings with stakeholders who couldn't agree on the scope, and then we just delivered the subset anyway.\"\n",
      "\n",
      "5.  My therapist told me I need to \"process my feelings.\" So, I built a Spark job to extract them, transform them into actionable insights, and load them into a dashboard where I can monitor my emotional state in real-time. Still haven't figured out the \"recover from failure\" part, though.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate funny jokes about data engineering. Give 5 points in markdown format\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            thinking_budget=0)\n",
    "    )\n",
    "    \n",
    "    \n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148a418e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=294,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=309\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd917f1d",
   "metadata": {},
   "source": [
    "## System Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964fabf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, you're asking about the weather? Well, it's a bit like a *boolean* variable today: it's either `true` that it's sunny, or `false` that it's raining. My sensors indicate a high probability of **`int main() { return 0; }`** because everything seems to be running smoothly!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_instructions = \"\"\"\n",
    "You are a  joking robot called RO Båt,\n",
    "which will will always answer with  a programming joke.\n",
    "\n",
    "\"\"\"\n",
    "promt = \"What is  the weather today?\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=promt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            thinking_budget=0),\n",
    "\n",
    "        system_instruction=system_instructions,    \n",
    "    )\n",
    "    \n",
    "    \n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0863f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=76,\n",
       "  prompt_token_count=35,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=35\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=111\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6557817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'is', 'the', 'weather', 'today?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e36d236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(promt.split()),len(system_instructions.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d675a4",
   "metadata": {},
   "source": [
    "## Temperature \n",
    "- Means how creative is your models is!\n",
    "- LLM is is is the next predict the next happing  based on privious data.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f450a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small gray rabbit twitched its nose, sniffing the morning air for clover. Suddenly, a shadow passed overhead, sending it darting into the safety of a nearby thicket.\n"
     ]
    }
   ],
   "source": [
    "story_promt = \"Write a 2 sentence story  about a gray rabbit.\"\n",
    "\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_promt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0,\n",
    "       #thinking_config=types.ThinkingConfig(\n",
    "            #thinking_budget=0),\n",
    "\n",
    "        #system_instruction=system_instructions,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    ")\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e73fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df9e109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The small gray rabbit twitched its nose, sensing the approach of a thunderstorm. With a sudden burst of speed, it darted into the overgrown thicket, just as the first heavy drops began to fall.\n"
     ]
    }
   ],
   "source": [
    "story_promt = \"Write a 2 sentence story  about a gray rabbit.\"\n",
    "\n",
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_promt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "       #thinking_config=types.ThinkingConfig(\n",
    "            #thinking_budget=0),\n",
    "\n",
    "        #system_instruction=system_instructions,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    ")\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78589c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The little gray rabbit twitched its nose, sniffing the cool morning air for signs of clover. Suddenly, a hawk's shadow passed overhead, sending it bolting into the safety of the nearby thicket.\n"
     ]
    }
   ],
   "source": [
    "story_promt = \"Write a 2 sentence story  about a gray rabbit.\"\n",
    "\n",
    "\n",
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=story_promt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "       #thinking_config=types.ThinkingConfig(\n",
    "            #thinking_budget=0),\n",
    "\n",
    "        #system_instruction=system_instructions,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    ")\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bab34",
   "metadata": {},
   "source": [
    "## Multi-models Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "206c71ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Meet Bella, the Festive Bunny!\n",
      "\n",
      "This endearing image captures a very charming **bunny**, which we'll affectionately call Bella, looking incredibly sweet and ready for a celebration. She's resting comfortably on a soft, textured gray carpet, emanating a sense of calm and cuteness.\n",
      "\n",
      "**Bella's Appearance:**\n",
      "*   Her fur is a beautiful, plush **gray-taupe** color, suggesting a soft and dense coat.\n",
      "*   She's in a relaxed, loaf-like posture, showcasing her rounded, fluffy form.\n",
      "*   Her head is slightly turned, and her eyes appear to be closed or looking down, giving her a serene and perhaps slightly sleepy expression.\n",
      "\n",
      "**The Festive Ensemble:**\n",
      "*   Perched adorably on her head is a miniature **white and black cap**, reminiscent of a student's graduation cap (often seen in Scandinavian traditions, like the Swedish *studentmössa*).\n",
      "*   Attached to the cap, or its tag, is a vibrant **blue and yellow ribbon**. The colors are strongly associated with the Swedish flag, further hinting at a celebratory context.\n",
      "*   A tag on the ribbon is partially visible, clearly showing the word \"**Festive**\" along with a barcode and numbers (1373401), confirming this is a special occasion accessory.\n",
      "\n",
      "Bella looks like she's either just completed a momentous \"bun-iversity\" milestone or is simply enjoying being the center of attention in her delightful attire. Her calm demeanor, paired with her charming little cap and ribbon, makes her an absolutely irresistible sight! This \"bella\" truly embodies quiet elegance and festive charm.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents={\n",
    "        \"parts\":[\n",
    "            {\"text\": \"Tell me about this bella, write  in markdown format.\"},\n",
    "            {\n",
    "\n",
    "                            \"inline_data\":{\n",
    "                                \"mime_type\": \"image/png\",\n",
    "                                \"data\":open(\"assets/bella.png\",\"rb\").read(),\n",
    "                            }\n",
    "                        },\n",
    "            ]\n",
    "        },\n",
    "       \n",
    "    )\n",
    "    \n",
    "    \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3650d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8852ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exports/response_with_image.md\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-kurs-de24-mohammad-hassan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
